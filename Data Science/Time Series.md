==TODO unfinished==

A time series is a dataset where the values are arranged in a sequence (doesn't actually have to be time)

We are concerned with predicting the next value in the sequence

## Filling in Missing Data

Missing data can be filled in with:
- Forward fill, where you use previous values to fill in the missing value, or
- Backward fill, where you use the next values to fill in the missing value

## Validation

> [!caution]
> Can't do normal cross-validation with time series

- Cut the time series off at a given point
- Predict the next period using mean squared error

## Time Series Components

- Noise: random jitters from things we can't see
	- Noise can be generated by any distribution, but is usually gaussian in nature
- Seasonality: Cyclic behavior
- Trend: Whether it's going up or down
	- A time series is **stationary** if it has a constant mean and variance (in which case it has no trend)

To find if time series is stationary, use Dickey-Fuller [hypothesis test](../Stats/Hypothesis%20and%20Inference.md)
- Dickey-Fuller checks if the time series has any **unit roots**
- **Unit roots** are a feature of time series that indicate if there is something affecting the data making it go away from the mean (seasonality or trend)

==TODO: Check out "Separating the components" slide==
### Separating the components

These 3 components can be combined in either an additive or multiplicative way:
- Additive: Model the target variable as the **sum** of the three components
	- i.e., $y_i = t_i + s_i + n_i$, where $t_i$, $s_i$, and $n_i$ are the trend, seasonality, and noise at each time step $i$
- Multiplicative: Model the target variable as the **product** of the three components
	- i.e., $y_i = t_i \cdot s_i \cdot n_i$

Steps for separating components according to https://timeseriesreasoning.com/contents/time-series-decomposition/ and https://machinelearningmastery.com/time-series-seasonality-with-python/:
- Use smoothing to get rid of seasonality and noise and get the trend component
	- Can use centered moving average for this
- Then, decide whether the composition is additive or multiplicative (see above for notes)
	- If additive, subtract trend from original time series
	- If multiplicative, divide original time series by trend
- Now what you have left is either $s_i + n_i$ or $s_i \cdot n_i$, depending on whether you assumed additive or multiplicative composition
- Guess the season length? Here, suppose it's a year
	- ==todo is there a way to do it without guessing?==
- Calculate the average $y$ for every January month, every February month, etc.
	- This assumes that the data across all of January is more or less the same (works for things like temperature)
	- Otherwise, you could get the average of every first day in a year, every second day in a year, etc.
- This gives you the pure seasonality component
- Again, remove the seasonality component from the combined $s_i + n_i$ or $s_i n_i$ data
- What you have left is the noise component

## Smoothing

- We want to remove the noise to find the "true" series
- This may be a better predictor than the actual data

### Modeling

==TODO: Take notes on this==

### Moving Average

==TODO: Take notes on this==

#### Moving Average with Exponential Smoothing

- $\alpha$ is a **smoothing factor** that takes values between 0 and 1
- It determines how fast the weight decreases for previous observations
- Don't want previous observations to contribute too much

#### Double Exponential Smoothing

- $\beta$ is the trend smoothing factor and takes values between 0 and 1
- ==TODO what is this for==

#### Triple Exponential Smoothing

==TODO takes notes on this==

## Modeling

### Auto-Regressive Model

The AR model only depends on past values (lags) to estimate future values

Takes 3 hyperparameters:
- p (lag order): number of lag observations in the model
- d (degree of differencing): number of times the raw observations are differenced
- q (order of the moving average): size of the moving average window